\begin{savequote}[75mm]
Some Quote.
\qauthor{Quoteauthor Lastname}
\end{savequote}

%For an example of a full page figure, see Fig.~\ref{fig:myFullPageFigure}.

\chapter{Conclusions}
\label{Chap:Conclusions}
\lettrine[lines=3, loversize=0.3]{\textcolor{DarkBlue}T}{hroughout this work,}  we have had one goal in mind; to develop video segmentation which has the reliability of tracking methods. The primary reason for doing this was to ensure the temporal consistency of segments through extended video clips of, in particular, indoor assembly tasks. Difficulties presented in such videos include partial and full occlusions, sudden and fast displacements of objects, different objects with similar or identical appearance, and objects which cannot be segmented based on color. Additionally, assembly tasks require a high degree of precision, particularly when it comes to the relative pose of parts when they are interacting (e.g. putting a bolt in a hole).

Our intended application for the segmentation of such videos was the general bootstrapping of assembly task understanding. If one can correctly track objects and their parts through a task without a-priori knowledge, it should be possible to use this to teach a robotic system from scratch. Not only this, but if one is able to correctly track full 6 DoF pose throughout a human-demonstrated assembly task, it is possible to learn trajectories that a robot can  use to directly imitate effective motion paths.

\section{Summary of Contributions}
We began in chapter \ref{Chap:VideoSegRelaxation} by presenting a 2D VOS method that made use of our proposed methodology; to use tracking as the basis for video segmentation. In particular, we showed how particle filters, a class of Sequential Bayesian Monte Carlo methods, can be used to predict what the next frame's segmentation should look like. These proposed segment masks were then combined using a weighted sampling strategy and then refined to fit observed image data using a relaxation process.

Next, in chapter \ref{Chap:WorldModel} we introduced RGB-D sensors, how they can be used to produce 3D point cloud data, and how this data can be organized using an octree structure. We then presented a specialized type of octree - the adjacency octree - which we developed to allow quick and efficient traversal between neighboring voxels within the tree. We subsequently showed how the adjacency octree can be used to efficiently further sub-divide voxel data into localized patches, called supervoxels, using our Voxel Cloud Connectivity Segmentation (VCCS). The utility of supervoxels was then demonstrated by showing their effectiveness in segmenting static scenes using a local convexity criterion (LCCP). The effectiveness of VCCS and LCCP were then demonstrated by showing their favorable results on a large state-of-the-art benchmark as compared to other state-of-the-art methods. Finally, we conclude the chapter by presenting how an adjacency octree containing supervoxels can be sequentially updated with new frames of data without deleting potentially occluded voxels.

We then extended the particle filter framework in chapter \ref{Chap:ModelBasedTracking} to 3D point clouds by formalizing the notion of a voxel-based measurement and dynamic model. While this straightforward implementation worked, we showed how it could achieve much faster (real-time on standard hardware) run times and accuracy by sampling correspondences. This improvement was then quantified using artificial data generated using a virtual reality simulator. As a final demonstration of the effectiveness of the tracker, we presented results on recordings of humans constructing the Cranfield benchmark. This showed how the tracker can be used to distill semantic understanding from a video sequence. 

Finally, in chapter \ref{Chap:TrackingBasedSegmentation} we tackled the problem of extracting full segmentations from tracked results. To do this, we first showed how the 3D particle filter presented previously could be extended to work on supervoxels. Then we showed how the adjacency graph of supervoxels could be used along with a global energy minimization to resolve interactions between trackers and produce a full segmentation consistent with the tracked poses. As a final demonstration of the presented methodology, we give results on several recordings of manipulation actions. 

Another important contribution was the development of the open-source Oculus vision system discussed in Appendix~\ref{chap:Oculus}. This system served as the platform on which much research has been done over the past several years and was a key tool in publications by several other researchers. Finally, we would like to note that most of the algorithms discussed in this work have been released as open-source to the vision community as part of the Point Cloud Library \footnote{\url{http://www.pointclouds.org/}}. We consider both of these important contributions, as the open sharing of code is vital to the advancement of the discipline of Computer Science.

\section{Shortcomings of VOS Benchmarks}
Evaluation of segmentation algorithms is a notoriously vexing problem due to the inherent ambiguity of what constitutes a ``correct'' segmentation. As such, in our work we have determined that we should avoid making concrete decisions on segmenting objects in single works, and instead chose to limit ourself to the lower, supervoxel level. While this is not an entirely satisfactory solution, we felt that there is simply not enough information in a single frame to extract meaningful segmentations accurately. Indeed, one cannot really tell the granularity with which a scene should be segmented into distinct objects until they see some action.

Benchmarking of 3D point cloud segmentation is a young topic - in fact, the first extensive benchmark, the \textit{NYU Indoor Dataset}, was not published until 2012 \cite{Silberman:ECCV12}. As such, it has many complications (that did not exist in 2D) which have yet to be resolved, such as that it is difficult, if not impossible, to make a 2D ground truth annotation correctly line up with the 3D point cloud representation. Even more to the point, there are still no 3D VOS benchmarks. In fact, even though the field is decades old, one must look to 2013 to find a 2D VOS benchmark \cite{Galasso2013}. There are many reasons for this lack of a proper benchmark, but the primary one is that is simply extremely time consuming to annotate ground-truth for even very short video sequences. Furthermore, labeling a single ground truth is even more difficult for video than single images, for instance, what happens when one takes a cap off of a bottle; should it be given a new label? If so, should it have had a separate label the entire time, or only once it is separated? What happens when objects become occluded and then reappear; should they be given new identities or maintain their old ones? If they keep their old ones, how long should we allow an object to be occluded for before we ``forget'' it?

Due to all of these concerns, we have made the decision in this work to benchmark our video segmentations by evaluating their ability to properly recognize actions. This, combined with qualitative results, effectively show the effectiveness of the method without the need to haggle over inscrutable questions such as ``what constitutes an object''?

\section{Limitations and Direction of Future Work}
The main limitation of the 2D tracking framework presented in chapter \ref{Chap:VideoSegRelaxation} is that it can only ``guess'' at correct behavior when an object is occluded. Indeed, this is a general problem of trying to infer behavior in a 3D world from observations in a 2D projected plane. It is because of these ambiguities and an inability to resolve them in a comprehensive and satisfying manner that we proceeded to tracking in 3D using RGB+D observations.

While our persistent voxel world model is very effective at maintaining the existence of stationary objects through occlusions, it does not handle objects which move while they are occluded. Solving this problem at the low-level of voxels is an unresolved problem, the outlook of which is fairly bleak. Our attempts at solving the problem lead us to believe that higher level object knowledge is necessary to account for occluded motion. With this in mind we are investigating a way of associating occluded objects with their occluder so that they move with them. Another limitation which we are currently addressing is that our persistent voxel world model does not account for camera motion. There is some existing work on real-time camera pose estimation, and we are hoping to incorporate such a method into our system in the near future.

As with any VOS method, our final result has a few important limitations. One of these is the need to set a rate at which to allow models to change. In many cases, objects are rigid, and do not change; allowing them to change only adds instability to the segmented output. Conversely, some objects are deformable, or not entirely visible in the scene, and will need to change to be correctly segmented. Another limitation is that we currently have no mechanism for adding new objects to the scene without reinitializing the tracking. Our current work focuses on correcting both of these limitations by incorporating the results of LCCP segmentation into the global minimization process to allow for parameter-free changing of models as well as the automated initialization of new tracks.






%% Requires fltpage2 package
%%
% \begin{FPfigure}
% \includegraphics[width=\textwidth]{figures/fullpage}
% \caption[Short figure name.]{This is a full page figure using the FPfigure command. It takes up the whole page and the caption appears on the preceding page. Its useful for large figures. Harvard's rules about full page figures are tricky, but you don't have to worry about it because we took care of it for you. For example, the full figure is supposed to have a title in the same style as the caption but without the actual caption. The caption is supposed to appear alone on the preceding page with no other text. You do't have to worry about any of that. We have modified the fltpage package to make it work. This is a lengthy caption and it clearly would not fit on the same page as the figure. Note that you should only use the FPfigure command in instances where the figure really is too large. If the figure is small enough to fit by the caption than it does not produce the desired effect. Good luck with your thesis. I have to keep writing this to make the caption really long. LaTex is a lot of fun. You will enjoy working with it. Good luck on your post doctoral life! I am looking forward to mine. \label{fig:myFullPageFigure}}
% \end{FPfigure}
% \afterpage{\clearpage}
